{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqQ_X51TXWeG"
      },
      "source": [
        "# Week 8 - Neuromorphic computing - Exercise\n",
        "\n",
        "Note: this is a new version of the exercise, for the old version see [w8-neuromorphic-exercise-v1.ipynb](w8-neuromorphic-exercise-v1.ipynb).\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuro4ml/exercises/blob/main/w8-neuromorphic/w8-neuromorphic-exercise.ipynb)\n",
        "\n",
        "## 🧠 Introduction\n",
        "\n",
        "Neuromorphic engineering is a field that aims to design and build artificial neural systems that mimic the architecture and principles of biological neural networks. Unlike traditional von Neumann computing architectures, neuromorphic chips:\n",
        "\n",
        "1. 🔄 Process information in a parallel, event-driven manner\n",
        "2. 💾 Integrate memory and computation\n",
        "3. ⚡ Operate with extremely low power consumption\n",
        "\n",
        "### 🤔 Why trade off power and accuracy?\n",
        "\n",
        "Traditional deep learning models running on GPUs or CPUs consume significant power (often hundreds of watts). In contrast, the human brain processes complex information while consuming only ~20 watts. Neuromorphic chips aim to bridge this efficiency gap by:\n",
        "\n",
        "- 📊 Using spike-based computation\n",
        "- 🎯 Implementing local learning rules\n",
        "- ⚡ Exploiting sparse, event-driven processing\n",
        "\n",
        "However, these benefits often come with reduced accuracy compared to traditional deep learning approaches. Understanding and optimizing this trade-off is crucial for deploying neural networks in power-constrained environments like mobile devices or IoT sensors.\n",
        "\n",
        "## 📝 Exercise overview\n",
        "\n",
        "In this exercise, you will:\n",
        "1. 🔧 Implement a simple neuromorphic chip simulator\n",
        "2. 🏃‍♂️ Train SNNs with different architectures\n",
        "3. 📊 Analyze the power-accuracy trade-off\n",
        "4. 🔍 Explore how different parameters affect this trade-off\n",
        "\n",
        "**This will also serve as a solid introduction on how to effectively train SNNs using modern packages such as SNNTorch!**\n",
        "\n",
        "## 💻 Setup\n",
        "\n",
        "Some of the code for this exercise is already provided, but you will need to implement some parts:\n",
        "\n",
        "### SNNModel (models.py)\n",
        "The `SNNModel` class implements a 2-layer Leaky Integrate-and-Fire (LIF) network using SNNTorch. The network architecture consists of:\n",
        "- Input layer → Hidden layer (with LIF neurons) → Output layer (with LIF neurons). (You will be able to play with other network architectures)\n",
        "- Each LIF neuron has a decay rate (beta) that controls how quickly the membrane potential decays. (You will be able to play with other neuron models provided by SNNTorch)\n",
        "- The network processes input data over multiple timesteps, producing spikes at each layer\n",
        "\n",
        "### NeuromorphicChip (chip.py)\n",
        "The `NeuromorphicChip` class simulates a neuromorphic hardware platform with the following constraints:\n",
        "- Maximum number of neurons: 1024\n",
        "- Maximum number of synapses: 64 * 1024\n",
        "- Memory per neuron: 32 bytes\n",
        "- Memory per synapse: 4 bytes\n",
        "- Energy consumption:\n",
        "  - 1e-1 nJ per neuron update\n",
        "  - 5e-4 nJ per synapse event\n",
        "  \n",
        "This backend hardware is very simple and does not include many features of neuromorphic hardware, and serves only as an introduction to thinking about efficient network design."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TERhNVM_XWeK"
      },
      "source": [
        "## Imports and data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "jap9-HTrXWeM",
        "outputId": "eaecee1e-188e-4d69-ea5c-33bcc7c6615f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: snntorch in /usr/local/lib/python3.10/dist-packages (0.9.1)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from snntorch) (2.5.1+cu121)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from snntorch) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from snntorch) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from snntorch) (1.26.4)\n",
            "Requirement already satisfied: nir in /usr/local/lib/python3.10/dist-packages (from snntorch) (1.0.4)\n",
            "Requirement already satisfied: nirtorch in /usr/local/lib/python3.10/dist-packages (from snntorch) (1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.1.0->snntorch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (2.8.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from nir->snntorch) (3.12.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->snntorch) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->snntorch) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->snntorch) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.1.0->snntorch) (3.0.2)\n",
            "fatal: destination path 'exercises' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install snntorch\n",
        "    !git clone https://github.com/neuro4ml/exercises.git\n",
        "    !cp exercises/w8-neuromorphic/*.py .\n",
        "    !cp exercises/w8-neuromorphic/dataset .\n",
        "    !cp exercises/w8-neuromorphic/dataset_labels .\n",
        "\n",
        "# If you are using a local machine, please install the dependencies yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "THQhZZiiXWeQ",
        "outputId": "0942daaa-d1f9-40ae-cf56-6fa912a598ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "# For automatic reloading of external modules\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "abTLbBLXXWeS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from chip import NeuromorphicChip\n",
        "from models import SNNModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL-XmFavXWeT"
      },
      "source": [
        "## 🛠️ Exercise 1.1: Mapping Implementation\n",
        "\n",
        "To complete this first question you need to implement the functions necessary to map your network on the chip.\n",
        "\n",
        "- 📍 Go to [models.py](models.py) and implement the `n_neurons` and `n_synapses` properties.\n",
        "- 📍 Go to [chip.py](chip.py) and implement the `calculate_memory_usage`, `map` and `run` methods.\n",
        "- ▶️ Run the following cell to check your implementation\n",
        "\n",
        "This is what you should see:\n",
        "\n",
        "    Simulation Results:\n",
        "    Energy consumption: 1.29 µJ\n",
        "    Memory usage: 57.34 KB\n",
        "    Total neuron updates: 11000\n",
        "    Total synapse events: 389740\n",
        "    Average spike rate: 0.205\n",
        "    Total spikes: 3070.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Ge1if7fjXWeV"
      },
      "outputs": [],
      "source": [
        "chip = NeuromorphicChip()\n",
        "\n",
        "dims = (128, 100, 10)\n",
        "n_timesteps = 100\n",
        "seed = 42\n",
        "snn = SNNModel(n_in=dims[0], n_hidden=dims[1], n_out=dims[-1], beta=0.95, seed=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Mxl02Ky1XWeV",
        "outputId": "059b363b-ed25-4e39-d6e3-5b65cc39d61e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Mapping not implemented",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-71231ca5a654>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Map the network on the chip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mchip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# Run the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/chip.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, snn)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \"\"\"\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapped_snn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mapping not implemented\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     def run(\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Mapping not implemented"
          ]
        }
      ],
      "source": [
        "# Generate random input (seed is fixed to 42 for reproducibility)\n",
        "torch.manual_seed(seed)\n",
        "input_data = torch.randn(n_timesteps, dims[0]) * 10  # 100 timesteps\n",
        "\n",
        "# Map the network on the chip\n",
        "chip.map(snn)\n",
        "# Run the network\n",
        "output, results = chip.run(input_data=input_data)\n",
        "\n",
        "print(\"\\nSimulation Results:\")\n",
        "print(f\"Energy consumption: {results['total_energy_nJ']/1000:.2f} µJ\")\n",
        "print(f\"Memory usage: {results['memory_usage_bytes']/1024:.2f} KB\")\n",
        "print(f\"Total neuron updates: {results['neuron_updates']}\")\n",
        "print(f\"Total synapse events: {results['synapse_events']}\")\n",
        "print(f\"Average spike rate: {results['spike_rate']:.3f}\")\n",
        "print(f\"Total spikes: {results['total_spikes']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T41oaZviXWeZ"
      },
      "source": [
        "## 🚫 Exercise 1.2: Failed Mappings\n",
        "\n",
        "Now let's explore what happens when we try to map networks that exceed the chip's constraints:\n",
        "\n",
        "### 🔬 Experiments:\n",
        "1. 🧠 First, we'll try mapping a network with too many neurons\n",
        "2. 🔗 Then, we'll attempt to map one with too many synapses\n",
        "3. 💡 Finally, we'll see how sparse connectivity can help fit larger networks\n",
        "\n",
        "Let's run these experiments and observe the error messages we get! Each case will demonstrate different limitations of neuromorphic hardware:\n",
        "The first two cases should return a `MemoryError` if your code is correct. The third case should run without errors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_aso1HXXWeb"
      },
      "outputs": [],
      "source": [
        "chip = NeuromorphicChip()\n",
        "\n",
        "# Case 1 : Too many neurons\n",
        "dims = (128, 1024, 10)\n",
        "seed = 42\n",
        "snn = SNNModel(n_in=dims[0], n_hidden=dims[1], n_out=dims[-1], beta=0.95, seed=seed)\n",
        "# Map the network on the chip\n",
        "try:\n",
        "    chip.map(snn)\n",
        "except MemoryError as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20e7YrLsXWed"
      },
      "outputs": [],
      "source": [
        "chip = NeuromorphicChip()\n",
        "\n",
        "# Case 2 : Too many synapses\n",
        "dims = (128, 512, 10)\n",
        "seed = 42\n",
        "snn = SNNModel(n_in=dims[0], n_hidden=dims[1], n_out=dims[-1], beta=0.95, seed=seed)\n",
        "# Map the network on the chip\n",
        "try:\n",
        "    chip.map(snn)\n",
        "except MemoryError as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcsqUDNAXWef"
      },
      "outputs": [],
      "source": [
        "# Case 3 : Sparse connectivity\n",
        "dims = (128, 512, 10)\n",
        "seed = 42\n",
        "snn = SNNModel(n_in=dims[0], n_hidden=dims[1], n_out=dims[-1], beta=0.95, seed=seed)\n",
        "for l in snn.layers:\n",
        "    if hasattr(l, \"weight\"):\n",
        "        l.weight.data = (\n",
        "            torch.rand(l.weight.data.shape) < 0.5\n",
        "        )  # 50% of the weights are non-zero\n",
        "\n",
        "# Map the network on the chip\n",
        "try:\n",
        "    chip.map(snn)\n",
        "    print(\n",
        "        f\"Mapped! Memory usage: {chip.calculate_memory_usage(snn)/1024:.2f} KB, Number of neurons: {snn.n_neurons}, Number of synapses: {snn.n_synapses}\"\n",
        "    )\n",
        "except MemoryError as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGT9oDcmXWeh"
      },
      "source": [
        "## 🎯 Exercise 2: Training\n",
        "\n",
        "In this exercise you will train a SNN on the [Randman dataset](https://github.com/fzenke/randman).\n",
        "\n",
        "### 📊 Background: The Randman Dataset\n",
        "\n",
        "The Randman dataset is a synthetic dataset specifically designed for training Spiking Neural Networks (SNNs). Here's what you need to know:\n",
        "\n",
        "1. **Dataset Structure**\n",
        "   - Generates labeled spike trains for classification\n",
        "   - Each sample consists of temporal spike patterns\n",
        "   - Data is organized into multiple classes (10 classes)\n",
        "   - Spike times are stored in `dataset` file\n",
        "   - Class labels are stored in `dataset_labels` file\n",
        "\n",
        "2. **Data Format**\n",
        "   - Input: Spike trains encoded as binary tensors (time x neurons)\n",
        "   - Each neuron can spike at different time steps\n",
        "   - Data is converted to one-hot encoding across time steps\n",
        "   - Shape: (batch_size, timesteps, input_neurons)\n",
        "\n",
        "3. **Classification Task**\n",
        "   - Goal: Classify input spike patterns into correct classes\n",
        "   - Output layer produces spike trains\n",
        "   - Classification is done using rate coding (for now !): the output neuron that spikes the most indicates the predicted class\n",
        "\n",
        "4. **Data Loading**\n",
        "   All necessary code for loading and preprocessing the data is provided:\n",
        "   - Data loading from files\n",
        "   - Conversion to one-hot encoding\n",
        "   - Train/test splitting\n",
        "   - DataLoader creation with batching\n",
        "\n",
        "### 🎓 2.1 Training\n",
        "\n",
        "- 📝 Go to [training.py](training.py) and complete the `SNNTrainer` class, in particular the `calculate_accuracy` method\n",
        "- ▶️ Run the following cell to train your network\n",
        "- 📊 Take a look at the training and testing metrics, especially the accuracy and energy consumption\n",
        "- 🔄 Start experimenting with different architectures and parameters to see how they affect the accuracy and energy consumption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0fEhMCpXWei"
      },
      "outputs": [],
      "source": [
        "from training import get_dataloaders, SNNTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uAKC996XWek"
      },
      "outputs": [],
      "source": [
        "# Create dataloaders\n",
        "train_loader, test_loader, dataset = get_dataloaders(\n",
        "    batch_size=64,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2uSPWebXWel"
      },
      "outputs": [],
      "source": [
        "# Take a look at the data\n",
        "data, labels = next(iter(train_loader))\n",
        "print(\n",
        "    data.shape, labels.shape\n",
        ")  # batch_size x timesteps x n_in. 1st and 2nd dims are swapped when passed to the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPjkgMa7XWel"
      },
      "outputs": [],
      "source": [
        "snn_config = {\n",
        "    \"n_hidden\": 128,\n",
        "    \"beta\": 0.95,\n",
        "    \"seed\": 42,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2oJDf4-XWeo"
      },
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "snn = SNNModel(\n",
        "    n_hidden=snn_config[\"n_hidden\"],\n",
        "    beta=snn_config[\"beta\"],\n",
        "    seed=snn_config[\"seed\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fU7JS4jDXWep"
      },
      "outputs": [],
      "source": [
        "# Initialize trainer\n",
        "trainer = SNNTrainer(snn, learning_rate=1e-3, lr_gamma=0.9, config=snn_config)\n",
        "# Train the model\n",
        "trainer.train(train_loader, test_loader, n_epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTo4nUZGXWep"
      },
      "source": [
        "### 📈 2.2 Plot the results\n",
        "- 📊 We can plot the accuracy and energy consumption as a function of the epoch\n",
        "- 📈 We see that the accuracy is improving but the energy consumption is also increasing\n",
        "- ⚖️ This is a trade-off that we need to be aware of when training SNNs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JB3rcirSXWeq"
      },
      "outputs": [],
      "source": [
        "results = trainer.pd_results.groupby(\"epoch\", as_index=False).mean()\n",
        "fig, ax = plt.subplots()\n",
        "sns.lineplot(\n",
        "    data=results, x=\"epoch\", y=\"accuracy\", ax=ax, label=\"Accuracy\", legend=False\n",
        ")\n",
        "ax2 = ax.twinx()\n",
        "sns.lineplot(\n",
        "    data=results,\n",
        "    x=\"epoch\",\n",
        "    y=\"total_energy_nJ\",\n",
        "    ax=ax2,\n",
        "    color=\"orange\",\n",
        "    label=\"Energy\",\n",
        "    legend=False,\n",
        ")\n",
        "ax.figure.legend()\n",
        "ax.set_title(\n",
        "    f\"Accuracy and Energy, Final Trade-off Score: {trainer.pareto_tradeoff:.2f}\"\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l5LJ9DAXWeq"
      },
      "source": [
        "## 🚀 Exercise 3: Optimizing the trade-off\n",
        "\n",
        "Now, you will explore how different parameters affect the accuracy and energy consumption of the SNN. This part is open-ended, here are some ideas:\n",
        "\n",
        "-  Experiment with network architectures (number of layers, number of neurons, etc.)\n",
        "-  Regularize spiking activity\n",
        "-  Implement a bi-exponential neuron model, using SnnTorch (snn.neurons.Synaptic)\n",
        "- Implement a temporal loss (time-to-first-spike), using SnnTorch. Be careful to change the `calculate_accuracy` method in `training.py`\n",
        "-  Implement weight masks to reduce the number of synapses\n",
        "-  Use SnnTorch to make the time-constants heterogeneous and/or learnable, and maybe use less neurons\n",
        "\n",
        "Ideally, after experimenting with these parameters, you should start to see a rough trade-off between accuracy and energy! Can we see some kind of Pareto front appearing?\n",
        "\n",
        "### 🏆 *The group with the best trade-off score will win the competition!*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "neuro4ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}